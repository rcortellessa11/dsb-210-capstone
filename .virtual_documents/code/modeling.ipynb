# Imports
import pandas as pd
import numpy as np

import ast
import re


df = pd.read_csv('../data/recipes_cleaned.csv')
df.head()


df['cleaned_ingredients'][0]





import ast
from nltk.stem import WordNetLemmatizer
import nltk

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to normalize and lemmatize each ingredient
def normalize_ingredient(ingredient):
    
    # Lemmatize each word in the ingredient
    words = ingredient.split()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    
    return ' '.join(lemmatized_words)

# Convert string to a real list of ingredients
df['cleaned_ingredients'] = df['cleaned_ingredients'].apply(ast.literal_eval)

# Now apply normalization and lemmatization to each ingredient in the list
df['normalized_ingredients'] = df['cleaned_ingredients'].apply(
    lambda ingr_list: [normalize_ingredient(i) for i in ingr_list]
)

# Example output
print(df['normalized_ingredients'].iloc[0])


def underscore_ingredients(ingredients):
    # Replace spaces in multi-word ingredients with underscores
    underscored = [ingredient.replace(' ', '_') for ingredient in ingredients]
    
    # Join into a space-separated string for TF-IDF
    return ' '.join(underscored)

# Apply the function to the 'normalized_ingredients' column
df['ingredients_str'] = df['normalized_ingredients'].apply(underscore_ingredients)


df['ingredients_str'][0]





from sklearn.feature_extraction.text import TfidfVectorizer


# Custom tokenizer that ensures multi-word ingredients remain with underscores
def custom_tokenizer(text):
    # Replace multi-word ingredient phrases with underscores and split by spaces
    # We assume the list is already in the proper format, where words like 'white_sugar' exist
    return text.split(' ')

# Instantiate
tfidf = TfidfVectorizer(tokenizer = custom_tokenizer)


# Fit
tfidf_matrix = tfidf.fit_transform(df['ingredients_str'])


# Get the feature names (words in the vocabulary)
feature_names = tfidf.get_feature_names_out()


print(feature_names[:20])


# Example pantry entry
user_input = ['butter', 'chocolate', 'white_sugar', 'eggs', 'vanilla_extract']
pantry_vector = tfidf.transform([' '.join(user_input)])


# Look at cosine similarity
from sklearn.metrics.pairwise import cosine_similarity


cosine_sim = cosine_similarity(pantry_vector, tfidf_matrix)


cosine_sim


cosine_sim_flat = cosine_sim.flatten()


# Get the indices of the top 5 most similar recipes
top_n_indices = cosine_sim_flat.argsort()[::-1][:5]


top_recipes = df.iloc[top_n_indices]


top_recipes[['title', 'intro', 'recipe_url']]





df.head()


# Split the ingredients string into tokens (words)
df['tokenized_ingredients'] = df['ingredients_str'].apply(lambda x: x.split())

# Check the tokenized ingredients
print(df['tokenized_ingredients'].head())


from gensim.models import Word2Vec # learns word vectors (embeddings) based on data (clean_ingredients)


# Train the Word2Vec model on tokenized ingredients
model = Word2Vec(sentences=df['tokenized_ingredients'], vector_size=100, window=5, min_count=1, workers=4)


# View vocabulary to see which words are included
vocab = list(model.wv.index_to_key)
print(vocab[:10])  # Display first 10 words in the vocabulary


word_vector = model.wv['butter']
print(word_vector)


similar_words = model.wv.most_similar('butter', topn=5)
print(similar_words)


# Function to compute average vector for each recipe
def get_average_embedding(tokens, model):
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

# Apply to each row
df['embedding'] = df['tokenized_ingredients'].apply(lambda tokens: get_average_embedding(tokens, model))


from numpy.linalg import norm

# Stack recipe vectors into a matrix
recipe_vectors = np.vstack(df['embedding'].values)


# Compute pairwise cosine similarity
cosine_sim = cosine_similarity(recipe_vectors)


# Example user ingredients
user_ingredients = ['butter', 'chocolate', 'white_sugar', 'eggs', 'vanilla_extract']
user_vector = get_average_embedding(user_ingredients, model)


user_vector = user_vector.reshape(1, -1)
similarities = cosine_similarity(user_vector, recipe_vectors)[0]


# Get indices of top 5 matches
top_indices = similarities.argsort()[::-1][:10]

# Show top recipes
top_recipes = df.iloc[top_indices][['title', 'ingredients_str']]
print(top_recipes)





df['ingredients_str']


def find_substitutable_matches_for_df(df, user_pantry):
    """
    This function checks for ingredient substitution matches based on word overlap.
    It processes the entire DataFrame of recipes and compares the ingredients with the user's pantry.

    Parameters:
    - df: The DataFrame containing recipes with ingredients as space-separated and underscore-joined.
    - user_pantry: List of ingredients the user has in their pantry.

    Returns:
    - df_with_substitutions: DataFrame with two new columns: 'matches' (substituted ingredients) and 'missing' (ingredients without any match).
    """
    
    def find_substitutable_matches(recipe_ingredients, user_pantry):
        matches = []
        missing = []
        
        recipe_list = recipe_ingredients.split()  # Split ingredients into words
        
        for ingredient in recipe_list:
            if ingredient in user_pantry:
                matches.append((ingredient, ingredient))  # Perfect match
            else:
                # Check word-level overlap
                ingredient_words = set(ingredient.split('_'))
                found = False
                for pantry_item in user_pantry:
                    pantry_words = set(pantry_item.split('_'))
                    if ingredient_words & pantry_words:  # Overlap between words
                        matches.append((ingredient, pantry_item))  # Substitution match
                        found = True
                        break
                if not found:
                    missing.append(ingredient)  # No match or substitution
        
        return matches, missing
    
    # Apply the substitution matching to each recipe in the dataframe
    df['matches'], df['missing'] = zip(*df['ingredients_str'].apply(lambda x: find_substitutable_matches(x, user_pantry)))
    
    return df


# Example user pantry
#user_pantry = ['butter', 'sugar', 'almond_milk', 'egg', 'cheese']

# Apply the substitution logic to the dataframe
#df_with_substitutions = find_substitutable_matches_for_df(df, user_pantry)

# See the new dataframe with 'matches' and 'missing' columns
#print(df_with_substitutions[['title','ingredients_str', 'matches', 'missing']].head())


df.head()


df.drop(columns = ['prep_time', 'cook_time', 'cleaned_ingredients', 'normalized_ingredients', 'ingredients_str'], inplace = True)


model_df = df


model_df.head()





import streamlit as st


# Create a python file for my app
smart_food_planner.py



