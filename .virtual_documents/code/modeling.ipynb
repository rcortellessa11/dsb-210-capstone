# Imports
import pandas as pd
import numpy as np

import ast
import re


df = pd.read_csv('../data/recipes_cleaned.csv')
df.head()


df['cleaned_ingredients'][0]





import ast
from nltk.stem import WordNetLemmatizer
import nltk

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to normalize and lemmatize each ingredient
def normalize_ingredient(ingredient):
    
    # Lemmatize each word in the ingredient
    words = ingredient.split()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    
    return ' '.join(lemmatized_words)

# Convert string to a list of ingredients
df['cleaned_ingredients'] = df['cleaned_ingredients'].apply(ast.literal_eval)

# Now apply normalization and lemmatization to each ingredient in the list
df['normalized_ingredients'] = df['cleaned_ingredients'].apply(
    lambda ingr_list: [normalize_ingredient(i) for i in ingr_list]
)

# Example output
df['normalized_ingredients'].iloc[0]


def underscore_ingredients(ingredients):
    # Replace spaces in multi-word ingredients with underscores
    underscored = [ingredient.replace(' ', '_') for ingredient in ingredients]
    
    # Join into a space-separated string 
    return ' '.join(underscored)

# Apply the function to the 'normalized_ingredients' column
df['ingredients_str'] = df['normalized_ingredients'].apply(underscore_ingredients)


df['ingredients_str'][0]





from sklearn.feature_extraction.text import TfidfVectorizer


# Custom tokenizer that ensures multi-word ingredients remain with underscores
def custom_tokenizer(text):
    # Replace multi-word ingredient phrases with underscores and split by spaces
    return text.split(' ')

# Instantiate
tfidf = TfidfVectorizer(tokenizer = custom_tokenizer)


# Fit
tfidf_matrix = tfidf.fit_transform(df['ingredients_str'])


# Get the feature names (words in the vocabulary)
feature_names = tfidf.get_feature_names_out()


print(feature_names[:20])


# Example pantry entry
user_input = ['butter', 'chocolate', 'white_sugar', 'eggs', 'vanilla_extract']
pantry_vector = tfidf.transform([' '.join(user_input)])


# Look at cosine similarity
from sklearn.metrics.pairwise import cosine_similarity


cosine_sim = cosine_similarity(pantry_vector, tfidf_matrix)


cosine_sim


cosine_sim_flat = cosine_sim.flatten()


# Get the indices of the top 5 most similar recipes
top_n_indices = cosine_sim_flat.argsort()[::-1][:5]


top_recipes = df.iloc[top_n_indices]


top_recipes[['title', 'intro', 'recipe_url']]





df.head()


# Split the ingredients string into tokens (words)
df['tokenized_ingredients'] = df['ingredients_str'].apply(lambda x: x.split())

# Check the tokenized ingredients
df['tokenized_ingredients'].head()


from gensim.models import Word2Vec # learns word vectors (embeddings) based on data (clean_ingredients)


# Train the Word2Vec model on tokenized ingredients
model = Word2Vec(sentences=df['tokenized_ingredients'], vector_size=100, window=5, min_count=1, workers=4)


# View vocabulary to see which words are included
vocab = list(model.wv.index_to_key)
vocab[:10] # Display first 10 words 


word_vector = model.wv['butter']
word_vector


similar_words = model.wv.most_similar('butter', topn=5)
similar_words


# Function to compute average vector for each recipe
def get_average_embedding(tokens, model):
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)

# Apply to each row
df['embedding'] = df['tokenized_ingredients'].apply(lambda tokens: get_average_embedding(tokens, model))


from numpy.linalg import norm

# Stack recipe vectors into a matrix
recipe_vectors = np.vstack(df['embedding'].values)


# Compute pairwise cosine similarity
cosine_sim = cosine_similarity(recipe_vectors)


# Example user ingredients
user_ingredients = ['butter', 'chocolate', 'white_sugar', 'eggs', 'vanilla_extract']
user_vector = get_average_embedding(user_ingredients, model)


user_vector = user_vector.reshape(1, -1)
similarities = cosine_similarity(user_vector, recipe_vectors)[0]


# Get indices of top 5 matches
top_indices = similarities.argsort()[::-1][:5]

# Show top recipes
top_recipes = df.iloc[top_indices][['title', 'ingredients_str']]
print(top_recipes)





df['ingredients_str']


def find_substitutable_matches_for_df(df, user_pantry):
    
    def find_substitutable_matches(recipe_ingredients, user_pantry):
        matches = []
        missing = []
        
        recipe_list = recipe_ingredients.split()  # Split ingredients into words
        
        for ingredient in recipe_list:
            if ingredient in user_pantry:
                matches.append((ingredient, ingredient))  # Perfect match
            else:
                # Check word-level overlap
                ingredient_words = set(ingredient.split('_'))
                found = False
                for pantry_item in user_pantry:
                    pantry_words = set(pantry_item.split('_'))
                    if ingredient_words & pantry_words:  # Overlap between words
                        matches.append((ingredient, pantry_item))  # Substitution match
                        found = True
                        break
                if not found:
                    missing.append(ingredient)  # No match or substitution
        
        return matches, missing
    
    # Apply the substitution matching to each recipe in the dataframe
    df['matches'], df['missing'] = zip(*df['ingredients_str'].apply(lambda x: find_substitutable_matches(x, user_pantry)))
    
    return df


# Example user pantry
user_pantry = ['butter', 'sugar', 'almond_milk', 'egg', 'cheese']

# Apply the substitution logic to the dataframe
#df_with_substitutions = find_substitutable_matches_for_df(df, user_pantry)

# df_with_substitutions[['title','ingredients_str', 'matches', 'missing']].head()


df.head()


df.drop(columns = ['prep_time', 'cook_time', 'cleaned_ingredients', 'normalized_ingredients'], inplace = True)


model_df = df


model_df.head()


df.to_csv('../data/model_df.csv', index=False)


model_df.shape


model_df.dtypes
